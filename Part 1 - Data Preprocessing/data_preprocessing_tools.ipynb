{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"37puETfgRzzg"},"source":["# Data Preprocessing Tools"]},{"cell_type":"markdown","metadata":{"id":"EoRP98MpR-qj"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"N-qiINBQSK2g"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Brief explanation of each library:\n","\n","1. **`numpy` (`np`)**:\n","   - Used for numerical computing in Python.\n","   - Provides support for large multi-dimensional arrays and matrices.\n","   - Includes functions for mathematical operations like linear algebra, statistical operations, and more.\n","\n","2. **`matplotlib.pyplot` (`plt`)**:\n","   - A plotting library for creating static, animated, and interactive visualizations in Python.\n","   - Commonly used to generate graphs, charts, and plots.\n","\n","3. **`pandas` (`pd`)**:\n","   - A library used for data manipulation and analysis.\n","   - Provides data structures like DataFrames, which are useful for handling and analyzing structured data, especially tabular data.\n","\n"],"metadata":{"id":"m5EXZPkwCHQ-"}},{"cell_type":"markdown","metadata":{"id":"RopL7tUZSQkT"},"source":["## Importing the dataset"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-rRF7rJTDMYU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893122286,"user_tz":-120,"elapsed":23391,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"90279c67-ab75-4a74-88d5-4344966577cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"WwEPNDWySTKm"},"source":["# Load the dataset from a CSV file into a pandas DataFrame\n","dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning A-Z/Part 1 - Data Preprocessing/Data.csv')\n","\n","# Extract all rows and all columns except the last one (features) into X\n","X = dataset.iloc[:, :-1].values # matrix of features\n","\n","# Extract all rows of the last column (target variable) into y\n","y = dataset.iloc[:, -1].values # dependent variable vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCsz2yCebe1R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893123961,"user_tz":-120,"elapsed":4,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"7cbe5a53-ed3e-4af0-ee6f-b0095282ba98"},"source":["print(X)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"code","metadata":{"id":"eYrOQ43XcJR3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893123961,"user_tz":-120,"elapsed":3,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"00f0d7ea-ffe8-49ff-fb9b-27c0f3576d53"},"source":["print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}]},{"cell_type":"markdown","metadata":{"id":"nhfKXNxlSabC"},"source":["## Taking care of missing data\n","\n","Handle missing values by replacing them with the mean of the respective columns:"]},{"cell_type":"code","metadata":{"id":"c93k7ipkSexq"},"source":["# Import the SimpleImputer class from sklearn\n","from sklearn.impute import SimpleImputer\n","\n","\n","# Create an imputer object that replaces missing values (np.nan) with the mean of the column\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","\n","# Fit the imputer to the specified columns (2nd and 3rd columns) of the dataset X\n","imputer.fit(X[:, 1:3])\n","\n","# Apply the transformation to replace missing values with the mean in those columns\n","X[:, 1:3] = imputer.transform(X[:, 1:3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UgLdMS_bjq_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893126721,"user_tz":-120,"elapsed":6,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"c46364c1-0b67-42e9-df30-95b4bdb545f1"},"source":["print(X)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","source":["The process of first fitting the `imputer` and then applying (transforming) it follows the typical machine learning pattern of **fitting** and **transforming**, which is necessary for the following reasons:\n","\n","1. **Fitting (`fit`)**: This step calculates and stores the necessary information needed to handle the missing values. In this case, the imputer calculates the mean of each column where there are missing values. The `fit` method identifies what value (e.g., the mean in this case) should be used to replace missing values.\n","\n","2. **Transforming (`transform`)**: Once the imputer has learned the necessary information (the mean values of the columns), the `transform` step actually applies the imputation. It replaces all the missing values with the calculated means from the fitting step.\n"],"metadata":{"id":"KNKVvzRIvGbj"}},{"cell_type":"markdown","metadata":{"id":"CriG6VzVSjcK"},"source":["## Encoding categorical data\n","\n","In machine learning, **categorical data** refers to features (or columns) that represent categories or labels, rather than numerical values. Examples include:\n","\n","- **Gender**: `Male`, `Female`\n","- **Country**: `USA`, `France`, `Germany`\n","- **Color**: `Red`, `Blue`, `Green`\n","\n","**Why Categorical Data Needs to Be Encoded:**\n","\n","Most machine learning algorithms require input data to be in numerical form because they rely on mathematical operations (e.g., distance calculations, dot products) that don't work on non-numeric (categorical) data. Therefore, we need to **encode** or convert these categorical variables into numbers.\n","\n","**Common Encoding Techniques:**\n","\n","1. **Label Encoding**:\n","   - Each category is assigned a unique integer. For example:\n","     - `Male` → `0`\n","     - `Female` → `1`\n","   - This method is simple but can introduce unintended ordinal relationships (i.e., algorithms might assume that `Female` > `Male` because of the numerical difference).\n","\n","2. **One-Hot Encoding**:\n","   - Each category is represented by a binary vector, with each unique category having its own column. For example, for the `Country` feature:\n","     - `USA` → `[1, 0, 0]`\n","     - `France` → `[0, 1, 0]`\n","     - `Germany` → `[0, 0, 1]`\n","   - This approach avoids any ordinal relationships and is commonly used in machine learning pipelines.\n","   \n","**Example of Why Encoding is Necessary:**\n","\n","Imagine trying to use a machine learning model with non-numeric values like this:\n","\n","| Age | Gender  | Country  |\n","|-----|---------|----------|\n","| 25  | Male    | USA      |\n","| 30  | Female  | France   |\n","| 22  | Female  | Germany  |\n","\n","If we pass the `Gender` and `Country` columns directly to the model, it won't be able to process strings like \"Male\" or \"France\". However, after encoding, the data would look something like this:\n","\n","| Age | Gender_Male | Gender_Female | Country_USA | Country_France | Country_Germany |\n","|-----|-------------|---------------|-------------|----------------|-----------------|\n","| 25  | 1           | 0             | 1           | 0              | 0               |\n","| 30  | 0           | 1             | 0           | 1              | 0               |\n","| 22  | 0           | 1             | 0           | 0              | 1               |\n","\n","Now the machine learning model can process the data since everything is in numerical form!"]},{"cell_type":"markdown","metadata":{"id":"AhSpdQWeSsFh"},"source":["### Encoding the Independent Variable\n","\n","We will use one-hot encoding."]},{"cell_type":"code","metadata":{"id":"5hwuVddlSwVi"},"source":["# Import the ColumnTransformer class to apply transformations to specific columns\n","from sklearn.compose import ColumnTransformer\n","\n","# Import the OneHotEncoder class to encode categorical variables as one-hot vectors\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Create a ColumnTransformer object that applies OneHotEncoder to the first column (index 0)\n","# 'remainder='passthrough'' means that the rest of the columns are left unchanged\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n","\n","# Fit the ColumnTransformer to the data and transform the first column using one-hot encoding\n","# Convert the result to a numpy array\n","X = np.array(ct.fit_transform(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7QspewyeBfx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893126721,"user_tz":-120,"elapsed":5,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"ade615b2-9fa5-402c-91dd-7daa35223401"},"source":["print(X)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"DXh8oVSITIc6"},"source":["### Encoding the Dependent Variable\n","\n","We will use label encoding."]},{"cell_type":"code","metadata":{"id":"XgHCShVyTOYY"},"source":["# Import the LabelEncoder class to convert categorical labels into numerical form\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Create an instance of LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit the LabelEncoder to the target variable (y) and transform it into numerical labels\n","# This converts categorical classes in y (e.g., 'Yes', 'No') into integers (e.g., 1, 0)\n","y = le.fit_transform(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyhY8-gPpFCa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893126721,"user_tz":-120,"elapsed":3,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"12894ec9-6761-4cf8-e0df-a7e4eea2b535"},"source":["print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1 0 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"qb_vcgm3qZKW"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","metadata":{"id":"pXgA6CzlqbCl"},"source":["# Import the train_test_split function to split the dataset into training and testing sets\n","from sklearn.model_selection import train_test_split\n","\n","# Split the dataset into training and testing sets\n","# X_train and y_train are the training sets; X_test and y_test are the testing sets\n","# test_size=0.2 means 20% of the data will be used for testing, and 80% for training\n","# random_state=1 ensures that the split is reproducible (same result every time you run it)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GuwQhFdKrYTM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893127525,"user_tz":-120,"elapsed":2,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"bbe19037-8851-48cf-b58f-f3de1dc36a91"},"source":["print(X_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"]}]},{"cell_type":"code","metadata":{"id":"TUrX_Tvcrbi4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893128502,"user_tz":-120,"elapsed":1,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"228e5c26-41b8-4159-eaee-6f47dc83e226"},"source":["print(X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 30.0 54000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"code","metadata":{"id":"pSMHiIsWreQY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893130362,"user_tz":-120,"elapsed":260,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"190b8195-d033-4741-f99d-4007894f3d3e"},"source":["print(y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1]\n"]}]},{"cell_type":"code","metadata":{"id":"I_tW7H56rgtW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893132248,"user_tz":-120,"elapsed":273,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"e08eaa78-52a1-4a4f-9630-7e37422a15d7"},"source":["print(y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"TpGqbS4TqkIR"},"source":["## Feature Scaling\n","\n","Feature scaling is a technique used to standardize the range of independent variables or features of data. The primary goal is to ensure that each feature contributes equally to the model's learning process, especially in algorithms that are sensitive to the scale of the features.\n","\n","**Why Feature Scaling is Important:**\n","\n","1. **Improves convergence**: Many machine learning algorithms, such as gradient descent-based methods (e.g., linear regression, logistic regression, neural networks), converge faster when features are on a similar scale.\n","2. **Prevents bias**: Algorithms that compute distances (e.g., K-nearest neighbors, clustering algorithms) can be biased towards features with larger ranges or magnitudes if features are not scaled.\n","3. **Enhances performance**: Scaling can improve the performance of some algorithms by ensuring that all features are given equal importance.\n","\n","**Common Methods of Feature Scaling:**\n","\n","1. **Standardization (Z-score Normalization)**:\n","   - **Formula**: \\( z = \\frac{x - \\mu}{\\sigma} \\)\n","   - **Explanation**: Transforms features to have a mean of 0 and a standard deviation of 1.\n","   - **When to use**: Useful when the features have different units or scales and the algorithm assumes normally distributed data.\n","    - It works well all the time.\n","\n","2. **Min-Max Normalization (Rescaling)**:\n","   - **Formula**: \\( x' = \\frac{x - \\text{min}}{\\text{max} - \\text{min}} \\)\n","   - **Explanation**: Transforms features to a fixed range, usually [0, 1].\n","   - **When to use**: Useful when features need to be scaled to a bounded range, such as for neural networks or algorithms that require a specific range.\n","    - Usually works well when we have a normal distribution.\n","\n","\n","**Do we have to do feature scaling before splitting the dataset into the training set and test set or after?**\n","\n","Feature scaling is generally performed **after** splitting the dataset into training and test sets. Here's why:\n","\n","1. **Avoid data leakage**: When you split your data, the test set is meant to represent unseen data. If you perform feature scaling (e.g., normalizing or standardizing) before splitting, information from the test set could \"leak\" into the training set because the scaling would have been influenced by data from both sets. This defeats the purpose of evaluating model performance on truly unseen data.\n","\n","2. **Scaling based on training data only**: You should fit the scaler (e.g., calculating the mean and standard deviation for standardization) on the training data only. Then, use the same transformation (based on the training data) on the test set. This simulates the real-world scenario where future data is scaled based on past data without recalculating scaling parameters.\n"]},{"cell_type":"code","metadata":{"id":"AxjSUXFQqo-3"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","# Create an instance of the StandardScaler\n","sc = StandardScaler()\n","\n","# Fit the scaler on the training data and transform the training features\n","# This calculates the mean and standard deviation from the training data\n","# and then scales the training data to have a mean of 0 and a standard deviation of 1\n","X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # only for values not in -3 and 3 range\n","\n","# Transform the test features using the scaler fitted on the training data\n","# This ensures that the test data is scaled using the same mean and standard deviation as the training data\n","X_test[:, 3:] = sc.transform(X_test[:, 3:]) # only transform, not fit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWPET8ZdlMnu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893370669,"user_tz":-120,"elapsed":245,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"1890469d-e11d-45b1-935d-3908a5d5422b"},"source":["print(X_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n"," [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n"," [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n"," [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n"," [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n"," [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n"," [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n"," [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"]}]},{"cell_type":"code","metadata":{"id":"sTXykB_QlRjE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725893373679,"user_tz":-120,"elapsed":274,"user":{"displayName":"Maialen Igartua","userId":"00826372013938757765"}},"outputId":"a316ae7e-7a73-4358-d282-a6401a594a79"},"source":["print(X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n"," [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"]}]}]}